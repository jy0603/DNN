{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch tutorial\n",
    "\n",
    "https://wikidocs.net/52460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [tensor 조작]\n",
    "\n",
    "1. 2차원 텐서의 크기= `(batch size 행의 크기x dimension 열의 크기)`<br>\n",
    "     - ex. (3000,256)의 텐서는 256차원 벡터가 3천개 모여있다\n",
    "     - 즉 가로 하나하나의 **행이 한 개의 data point이다.**\n",
    "     - 컴퓨터는 batch크기=64개씩 꺼내서 처리한다면 이 때 batch size=64\n",
    "\n",
    "\n",
    "2. 3차원 텐서의 크기= `(batch size 행 x width 열 x height 행렬갯수 )` <br>\n",
    "     - batch size x width의 행렬이 height개 만큼 뒤로 쌓여있다.\n",
    "     - nlp에서: (batch size, length, dim)=(문장의 총 갯수, 문장 당 단어의 수, 각 단어벡터 차원) \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp tensor\n",
    "# 1문장 당 단어 3개, 총 4문장 -> 4 x 3 텐서 (2d)\n",
    "# 한 단어를 3차원 벡터로 만든다. ==> 4 x 3 x 3 텐서 ( 2d가 뒤로 3개 쌓인 것!)\n",
    "# (4개 문장, 각문장 당 단어수는 3개, 각단어 당 3차원 vector) \n",
    "# batch size=2라고 함은 한번에 컴퓨터가 2개 문장을 처리한다는 뜻\n",
    "# 그렇다면 각 배치의 텐서 크기는 (2x3x3)\n",
    "\n",
    "nlp=np.array([[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],\n",
    " [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],\n",
    " [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],\n",
    " [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]])\n",
    "\n",
    "nlp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "# 1d torch\n",
    "\n",
    "t= torch.FloatTensor([0.,1.,2.,3.,4.,5.,6.])\n",
    "print(t)\n",
    "print(t.dim())  # rank. 즉, 차원\n",
    "print(t.shape)  # shape\n",
    "print(t.size()) # shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(t[0], t[1], t[-1])  # 인덱스로 접근\n",
    "print(t[2:5], t[4:-2])    # 슬라이싱\n",
    "print(t[:2], t[3:])       # 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 2d torch\n",
    "\n",
    "t=torch.FloatTensor([[1.,2.,3.],\n",
    "                     [4.,5.,6.],\n",
    "                     [7., 8., 9.],\n",
    "                     [10., 11., 12.]                    \n",
    "                    \n",
    "                    ])\n",
    "\n",
    "print(t)\n",
    "print(t.dim())  # rank. 즉, 차원\n",
    "print(t.size()) # shape\n",
    "print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.\n",
    "print(t[:, 1].size()) # ↑ 위의 경우의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n",
      "tensor([[4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# broadcasting= 2개 텐서 사이 연산을 가능하게 자동으로 크게 맞추는 기능\n",
    "\n",
    "#1. 같은 크기\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1+m2)\n",
    "\n",
    "#2. 다른 크기 -> broadcasting 필요\n",
    "# Vector + scalar\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3]) # [3] -> [3, 3]\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 1 Vector + 1 x 2 Vector\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([[3], [4]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. matmul(행렬 곱셈) vs mul(원소 별 곱셈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "# 행렬 곱셈\n",
    "\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1.matmul(m2)) # 2 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 다른 크기의 행렬이 브로드캐스팅되어서 크기가 서로 같게 만들어준다.-> 각 원소 자리 별 연산\n",
    "\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1 --> 얘를 2 x2로 브로드케스팅해준 후 계산한다. \n",
    "print(m1 * m2) # 2 x 2\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dim=0 은 열마다 계산, dim=1은 행마다 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n",
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 평균 ( dim=0/1)\n",
    "\n",
    "t=torch.FloatTensor([1,2])\n",
    "print(t.mean())\n",
    "\n",
    "# 2차원 평균\n",
    "\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t.mean()) # 4개 모두의 평균\n",
    "\n",
    "print(t.mean(dim=0)) # dim을 주는 경우, dim=0은 첫번째 차원인 '행'차원을 제거한다. -> 열마다 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "tensor([1, 1])\n",
      "=============================\n",
      "Max:  tensor([3., 4.])\n",
      "Argmax:  tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t.max(dim=0)) # 열을 기준으로 max를 각각 찾는다. \n",
    "# values: 각 열에서의 최대값은 3,4이며\n",
    "# indices:각 열에서 최대값이 위치하는 인덱스는 1번째, 1번째이다. (matrix에선 각 열의 2번째 위치)\n",
    "print(t.argmax(dim=0)) # same as indices \n",
    "\n",
    "# 즉, \n",
    "print('=============================')\n",
    "print('Max: ', t.max(dim=0)[0])\n",
    "print('Argmax: ', t.max(dim=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [텐서 조작2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# view: 원소의 수를 유지하면서 텐서 크기를 변경한다. = np.reshape\n",
    "\n",
    "\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "# t.shape=(2,2,3) --> 2개(행)의 문장이 있고, 각 문장 당 2개의 단어가, 각 단어는 3차원이다. \n",
    "\n",
    "ft=torch.FloatTensor(t)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view 함수 내부에 -1로 dimension을 지정했으면, 다른 dim으로부터 유추하여 알아서 디멘션을 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3차원->2차원 텐서\n",
    "\n",
    "print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경\n",
    "print(ft.view([-1, 3]).shape) # 위의 ft와 원소수는 12개로 동일하나, shape만 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "==================================\n",
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 텐서의 크기 변경\n",
    "\n",
    "print(ft)\n",
    "print('================='*2)\n",
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# squeeze: 차원수가 =1인 차원을 제거한다.\n",
    "\n",
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)\n",
    "\n",
    "# 2번째 차원이 1이므로 이를 없애준다. ->1차원 벡터 \n",
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze: 특정 위치에 차원수가 1인 차원을 추가한다.( 위와 반대! )\n",
    "\n",
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft.shape) # 차원이 1개인 1차원 벡터이다. 앞에 1차원을 추가해보자.\n",
    "\n",
    "print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
    "print(ft.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view(1, -1)) # 이렇게도 가능함.\n",
    "print(ft.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(-1))\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# type casting: 자료형 변환\n",
    "\n",
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)\n",
    "print(lt.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt)\n",
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 2개 텐서의 연결\n",
    "\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "print(torch.cat([x, y], dim=0)) # dim=0: 첫번째 차원을 늘린다. ->> 2개를 위아래로 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x, y], dim=1)) # 2개를 옆으로 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# stacking: 쌓는 것\n",
    "\n",
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(torch.stack([x, y, z]))\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0)) # same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x, y, z], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 모든 원소가 0,1\n",
    "\n",
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)\n",
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 덮어쓰기 연산; 연산 수행 후 다시 객체에 할당안해도 언더바를 연산함수 뒤에 붙이면 바로\n",
    "# 결과값이 할당이 된다. inplace=True와 같은 의미이다. \n",
    "\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력\n",
    "print(x) # 기존의 값 출력\n",
    "\n",
    "print(x.mul_(2.))  # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력\n",
    "print(x) # 기존의 값 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [선형회귀]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e792e8ad80>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 변수 선언\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 가중치 초기화\n",
    "W = torch.zeros(1, requires_grad=True)  # requires_grad: 학습을 통해 값이 변경되는 변수임을 명시.\n",
    "print(W) \n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b # w,b는 x_train 섀입에 맞게 브로드케스팅 된 후 계산된다. \n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
    "cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 객체 생성\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad() \n",
    "\n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward() \n",
    "\n",
    "# W와 b를 업데이트\n",
    "optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch: 전체 full train data가 학습에 몇 번 사용되었나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 0으로 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1): # 2천번동안 파라메터를 업데이트한다.\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()f # 파이토치는 미분값을 계속누적하니까, 한 에폭 돌때마다 초기화 필요.\n",
    "    cost.backward() # requires_grad=True한 w,b에 미분값을 저장한다.\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [자동 미분(Autograd)]\n",
    "\n",
    "\n",
    "requires_grad=True, backward() ...?? <br>https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html?highlight=autograd<br>\n",
    "\n",
    "\n",
    "- 자동 미분을 사용하면 미분 계산을 자동화하여 경사 하강법을 손쉽게 사용할 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True) # # requires_grad=True한 w에 미분값을 저장한다.\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "y = w**2\n",
    "z = 2*y + 5\n",
    "\n",
    "z.backward() # z의 도함수(미분)을 계산하고, requires_grad가 되어있는 w에 저장한다.\n",
    "\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001E796EC8278>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # false\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # true\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward() # 역전파 계산\n",
    "\n",
    "print(x.grad) # out를 x에 대해 미분한다. 왜냐면 앞서 x에 requires_grad를 지정했기 때문.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040649\n",
      "Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936157\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445281\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710555\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621262\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619764\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"
     ]
    }
   ],
   "source": [
    "# 다중 선형 회귀 : x변수가 여러개.\n",
    "\n",
    "\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])   # (5 x 3) 5개의 sample. 따라서 결과도 5개\n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]]) # (5,1)\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True) # x train과 행렬곱 가능한 size.\n",
    "b = torch.zeros(1, requires_grad=True) # 브로드캐스팅 to (5,1)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 초기화\n",
    "    cost.backward() # cost함수를 w,b에 대하여 미분\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ nn으로 구현]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e792e8ad80>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#model = nn.Linear(input_dim, output_dim) # 선형회귀모델을 구현한 함수 nn클래스의 Linear함수!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) # (3,1) 즉 3개의 데이터, 1개의 x변수/\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 모델을 선언 및 초기화. 단순 선형 회귀, 즉 x가 1개이므로, 1개의 입력에 대해 1개의 output.\n",
    "model = nn.Linear(1,1) # 선형회귀모델\n",
    "print(list(model.parameters())) # w,b가 내장되어 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "\n",
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0014], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) # 학습 후의 파라메터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1119,  0.2710, -0.5435]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3462], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 42134.707031\n",
      "Epoch  100/2000 Cost: 5.960090\n",
      "Epoch  200/2000 Cost: 5.654707\n",
      "Epoch  300/2000 Cost: 5.365413\n",
      "Epoch  400/2000 Cost: 5.091413\n",
      "Epoch  500/2000 Cost: 4.831860\n",
      "Epoch  600/2000 Cost: 4.585968\n",
      "Epoch  700/2000 Cost: 4.353065\n",
      "Epoch  800/2000 Cost: 4.132451\n",
      "Epoch  900/2000 Cost: 3.923481\n",
      "Epoch 1000/2000 Cost: 3.725512\n",
      "Epoch 1100/2000 Cost: 3.537961\n",
      "Epoch 1200/2000 Cost: 3.360339\n",
      "Epoch 1300/2000 Cost: 3.192072\n",
      "Epoch 1400/2000 Cost: 3.032686\n",
      "Epoch 1500/2000 Cost: 2.881712\n",
      "Epoch 1600/2000 Cost: 2.738675\n",
      "Epoch 1700/2000 Cost: 2.603187\n",
      "Epoch 1800/2000 Cost: 2.474835\n",
      "Epoch 1900/2000 Cost: 2.353291\n",
      "Epoch 2000/2000 Cost: 2.238128\n",
      "[Parameter containing:\n",
      "tensor([[0.8541, 0.8475, 0.3096]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3568], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "## 다중 선형 회귀 구현\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]]) # (5,3) - 5개 데이터, 3개 변수\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "model = nn.Linear(3,1)\n",
    "\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))\n",
    "        \n",
    "        \n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [class로 nn모델 구현]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 17.088327\n",
      "Epoch  100/2000 Cost: 0.030164\n",
      "Epoch  200/2000 Cost: 0.018639\n",
      "Epoch  300/2000 Cost: 0.011518\n",
      "Epoch  400/2000 Cost: 0.007117\n",
      "Epoch  500/2000 Cost: 0.004398\n",
      "Epoch  600/2000 Cost: 0.002718\n",
      "Epoch  700/2000 Cost: 0.001679\n",
      "Epoch  800/2000 Cost: 0.001038\n",
      "Epoch  900/2000 Cost: 0.000641\n",
      "Epoch 1000/2000 Cost: 0.000396\n",
      "Epoch 1100/2000 Cost: 0.000245\n",
      "Epoch 1200/2000 Cost: 0.000151\n",
      "Epoch 1300/2000 Cost: 0.000094\n",
      "Epoch 1400/2000 Cost: 0.000058\n",
      "Epoch 1500/2000 Cost: 0.000036\n",
      "Epoch 1600/2000 Cost: 0.000022\n",
      "Epoch 1700/2000 Cost: 0.000014\n",
      "Epoch 1800/2000 Cost: 0.000008\n",
      "Epoch 1900/2000 Cost: 0.000005\n",
      "Epoch 2000/2000 Cost: 0.000003\n"
     ]
    }
   ],
   "source": [
    "# 클래스로 단순선형회귀모델을 구현한다.\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "class LinearRegressionMd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() ##nn.Module의 구조를 다 상속받음\n",
    "        self.linear=nn.Linear(1,1) # 선형회귀 모형 선언\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x) # prediction = model(x_train) 이부분, model==self.linear\n",
    "    \n",
    "model=LinearRegressionMd()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [batch size]\n",
    "\n",
    "- 전체 데이터 수는 2천개, 배치 사이즈=200개씩, 따라서 iteration=10번\n",
    "- iteration 수 만큼 가중치가 업데이트된다. \n",
    "- 10번 업데이트되어야 전체 훈련 데이터를 다 쓴 1 epoch이 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x000001E796E92198>\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(x_train, y_train)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch    0/20 Batch 1/3 Cost: 3225.263916\n",
      "1\n",
      "Epoch    0/20 Batch 2/3 Cost: 1182.824951\n",
      "2\n",
      "Epoch    0/20 Batch 3/3 Cost: 187.361023\n",
      "0\n",
      "Epoch    1/20 Batch 1/3 Cost: 95.714813\n",
      "1\n",
      "Epoch    1/20 Batch 2/3 Cost: 56.367149\n",
      "2\n",
      "Epoch    1/20 Batch 3/3 Cost: 21.278282\n",
      "0\n",
      "Epoch    2/20 Batch 1/3 Cost: 4.333995\n",
      "1\n",
      "Epoch    2/20 Batch 2/3 Cost: 3.526613\n",
      "2\n",
      "Epoch    2/20 Batch 3/3 Cost: 0.207875\n",
      "0\n",
      "Epoch    3/20 Batch 1/3 Cost: 0.305666\n",
      "1\n",
      "Epoch    3/20 Batch 2/3 Cost: 3.576965\n",
      "2\n",
      "Epoch    3/20 Batch 3/3 Cost: 2.601593\n",
      "0\n",
      "Epoch    4/20 Batch 1/3 Cost: 0.990082\n",
      "1\n",
      "Epoch    4/20 Batch 2/3 Cost: 2.511539\n",
      "2\n",
      "Epoch    4/20 Batch 3/3 Cost: 5.467659\n",
      "0\n",
      "Epoch    5/20 Batch 1/3 Cost: 2.074741\n",
      "1\n",
      "Epoch    5/20 Batch 2/3 Cost: 1.362740\n",
      "2\n",
      "Epoch    5/20 Batch 3/3 Cost: 3.012716\n",
      "0\n",
      "Epoch    6/20 Batch 1/3 Cost: 3.302794\n",
      "1\n",
      "Epoch    6/20 Batch 2/3 Cost: 1.413140\n",
      "2\n",
      "Epoch    6/20 Batch 3/3 Cost: 0.515133\n",
      "0\n",
      "Epoch    7/20 Batch 1/3 Cost: 1.423153\n",
      "1\n",
      "Epoch    7/20 Batch 2/3 Cost: 2.968907\n",
      "2\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.878935\n",
      "0\n",
      "Epoch    8/20 Batch 1/3 Cost: 2.962180\n",
      "1\n",
      "Epoch    8/20 Batch 2/3 Cost: 0.939499\n",
      "2\n",
      "Epoch    8/20 Batch 3/3 Cost: 3.404086\n",
      "0\n",
      "Epoch    9/20 Batch 1/3 Cost: 2.246527\n",
      "1\n",
      "Epoch    9/20 Batch 2/3 Cost: 4.213487\n",
      "2\n",
      "Epoch    9/20 Batch 3/3 Cost: 1.282365\n",
      "0\n",
      "Epoch   10/20 Batch 1/3 Cost: 1.101204\n",
      "1\n",
      "Epoch   10/20 Batch 2/3 Cost: 2.426246\n",
      "2\n",
      "Epoch   10/20 Batch 3/3 Cost: 2.538969\n",
      "0\n",
      "Epoch   11/20 Batch 1/3 Cost: 1.782012\n",
      "1\n",
      "Epoch   11/20 Batch 2/3 Cost: 0.902609\n",
      "2\n",
      "Epoch   11/20 Batch 3/3 Cost: 6.496058\n",
      "0\n",
      "Epoch   12/20 Batch 1/3 Cost: 0.967940\n",
      "1\n",
      "Epoch   12/20 Batch 2/3 Cost: 2.198942\n",
      "2\n",
      "Epoch   12/20 Batch 3/3 Cost: 5.569603\n",
      "0\n",
      "Epoch   13/20 Batch 1/3 Cost: 0.967736\n",
      "1\n",
      "Epoch   13/20 Batch 2/3 Cost: 3.045833\n",
      "2\n",
      "Epoch   13/20 Batch 3/3 Cost: 1.437577\n",
      "0\n",
      "Epoch   14/20 Batch 1/3 Cost: 3.064418\n",
      "1\n",
      "Epoch   14/20 Batch 2/3 Cost: 0.503609\n",
      "2\n",
      "Epoch   14/20 Batch 3/3 Cost: 2.996529\n",
      "0\n",
      "Epoch   15/20 Batch 1/3 Cost: 1.449248\n",
      "1\n",
      "Epoch   15/20 Batch 2/3 Cost: 3.300322\n",
      "2\n",
      "Epoch   15/20 Batch 3/3 Cost: 3.913437\n",
      "0\n",
      "Epoch   16/20 Batch 1/3 Cost: 1.816159\n",
      "1\n",
      "Epoch   16/20 Batch 2/3 Cost: 2.689441\n",
      "2\n",
      "Epoch   16/20 Batch 3/3 Cost: 0.566467\n",
      "0\n",
      "Epoch   17/20 Batch 1/3 Cost: 0.614407\n",
      "1\n",
      "Epoch   17/20 Batch 2/3 Cost: 3.264152\n",
      "2\n",
      "Epoch   17/20 Batch 3/3 Cost: 2.287970\n",
      "0\n",
      "Epoch   18/20 Batch 1/3 Cost: 2.355929\n",
      "1\n",
      "Epoch   18/20 Batch 2/3 Cost: 0.954182\n",
      "2\n",
      "Epoch   18/20 Batch 3/3 Cost: 3.174211\n",
      "0\n",
      "Epoch   19/20 Batch 1/3 Cost: 0.330994\n",
      "1\n",
      "Epoch   19/20 Batch 2/3 Cost: 3.468718\n",
      "2\n",
      "Epoch   19/20 Batch 3/3 Cost: 2.508378\n",
      "0\n",
      "Epoch   20/20 Batch 1/3 Cost: 3.048748\n",
      "1\n",
      "Epoch   20/20 Batch 2/3 Cost: 1.198771\n",
      "2\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.879822\n"
     ]
    }
   ],
   "source": [
    "# 데이터로더의 파라메터: dataset, 미니배치 사이즈. \n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "         print(batch_idx)\n",
    "         #print(samples)\n",
    "         x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "         prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "         cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "         optimizer.zero_grad()\n",
    "         cost.backward()\n",
    "         optimizer.step()\n",
    "\n",
    "         print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [custom dataset 만들기]\n",
    "\n",
    "1. torch.utils.data.Dataset은 데이터셋을 나타내는 추상클래스이다.\n",
    "2. 우리의 데이터셋은 `Dataset`클래스를 상속받아야 하며, 아래 2가지 함수를 추가 필요 한다.\n",
    "    - `__init__`을 이용하여 csv파일 내의 데이터를 읽는다.\n",
    "    - `len(dataset)`에서 호출되는 `__len__`: 데이터셋 크기 리턴\n",
    "    - `dataset[i]`에서 호출되는 `__getitem__`: i번째 데이터 샘플을 찾는다.\n",
    "    \n",
    "    \n",
    "3. DataLoader 클래스는 mini batch를 만들어준다. 이 클래스를 통해 dataset 전체가 베치 사이즈로 slice된다. (알아서 배치 데이터 만들어 줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset() # 데이터 개수, x,y를 리턴한다.\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) # 2개씩 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x0000014970392208>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index is   0\n",
      "samples:  [tensor([[ 73.,  66.,  70.],\n",
      "        [ 96.,  98., 100.]]), tensor([[142.],\n",
      "        [196.]])]\n",
      "Epoch    0/20 Batch 1/3 Cost: 14382.714844\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch    0/20 Batch 2/3 Cost: 4313.676758\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch    0/20 Batch 3/3 Cost: 1909.010986\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[73., 66., 70.],\n",
      "        [93., 88., 93.]]), tensor([[142.],\n",
      "        [185.]])]\n",
      "Epoch    1/20 Batch 1/3 Cost: 375.639832\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch    1/20 Batch 2/3 Cost: 110.891167\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    1/20 Batch 3/3 Cost: 61.624992\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch    2/20 Batch 1/3 Cost: 10.449733\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch    2/20 Batch 2/3 Cost: 0.955104\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch    2/20 Batch 3/3 Cost: 4.999662\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch    3/20 Batch 1/3 Cost: 0.957679\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch    3/20 Batch 2/3 Cost: 0.202829\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.973415\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch    4/20 Batch 1/3 Cost: 0.646236\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch    4/20 Batch 2/3 Cost: 0.856059\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    4/20 Batch 3/3 Cost: 0.007454\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.521543\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch    5/20 Batch 2/3 Cost: 0.690411\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch    5/20 Batch 3/3 Cost: 0.688802\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.504762\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch    6/20 Batch 2/3 Cost: 1.146346\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    6/20 Batch 3/3 Cost: 0.008001\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]), tensor([[180.],\n",
      "        [196.]])]\n",
      "Epoch    7/20 Batch 1/3 Cost: 0.369836\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch    7/20 Batch 2/3 Cost: 1.184856\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.181836\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch    8/20 Batch 1/3 Cost: 0.268088\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch    8/20 Batch 2/3 Cost: 0.391970\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch    8/20 Batch 3/3 Cost: 1.370789\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [93., 88., 93.]]), tensor([[152.],\n",
      "        [185.]])]\n",
      "Epoch    9/20 Batch 1/3 Cost: 0.816749\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch    9/20 Batch 2/3 Cost: 0.246556\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch    9/20 Batch 3/3 Cost: 1.044896\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch   10/20 Batch 1/3 Cost: 1.416806\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch   10/20 Batch 2/3 Cost: 0.615171\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.013420\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch   11/20 Batch 1/3 Cost: 0.904963\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[ 73.,  66.,  70.],\n",
      "        [ 96.,  98., 100.]]), tensor([[142.],\n",
      "        [196.]])]\n",
      "Epoch   11/20 Batch 2/3 Cost: 0.273539\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch   11/20 Batch 3/3 Cost: 0.099554\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch   12/20 Batch 1/3 Cost: 0.390217\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch   12/20 Batch 2/3 Cost: 0.463098\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   12/20 Batch 3/3 Cost: 1.057035\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]), tensor([[180.],\n",
      "        [196.]])]\n",
      "Epoch   13/20 Batch 1/3 Cost: 1.080728\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch   13/20 Batch 2/3 Cost: 0.288514\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   13/20 Batch 3/3 Cost: 0.822839\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.252730\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch   14/20 Batch 2/3 Cost: 0.326599\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch   14/20 Batch 3/3 Cost: 1.745764\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch   15/20 Batch 1/3 Cost: 0.614243\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch   15/20 Batch 2/3 Cost: 0.716028\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   15/20 Batch 3/3 Cost: 0.022967\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch   16/20 Batch 1/3 Cost: 0.746014\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch   16/20 Batch 2/3 Cost: 0.943095\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   16/20 Batch 3/3 Cost: 0.000456\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [93., 88., 93.]]), tensor([[152.],\n",
      "        [185.]])]\n",
      "Epoch   17/20 Batch 1/3 Cost: 0.556298\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[ 73.,  66.,  70.],\n",
      "        [ 96.,  98., 100.]]), tensor([[142.],\n",
      "        [196.]])]\n",
      "Epoch   17/20 Batch 2/3 Cost: 0.202605\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch   17/20 Batch 3/3 Cost: 1.200733\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch   18/20 Batch 1/3 Cost: 0.465451\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch   18/20 Batch 2/3 Cost: 0.627247\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch   18/20 Batch 3/3 Cost: 1.101577\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]), tensor([[180.],\n",
      "        [196.]])]\n",
      "Epoch   19/20 Batch 1/3 Cost: 0.193321\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch   19/20 Batch 2/3 Cost: 0.525586\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   19/20 Batch 3/3 Cost: 1.362654\n",
      "========================================\n",
      "batch_index is   0\n",
      "samples:  [tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch   20/20 Batch 1/3 Cost: 0.252715\n",
      "========================================\n",
      "batch_index is   1\n",
      "samples:  [tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch   20/20 Batch 2/3 Cost: 1.401409\n",
      "========================================\n",
      "batch_index is   2\n",
      "samples:  [tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.030237\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "\n",
    "# 5개 데이터, batch size=2개씩\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        print('batch_index is  ',batch_idx)\n",
    "        print('samples: ',samples)\n",
    "        \n",
    "        x_train, y_train = samples # 2개의 데이터를 x,y훈련데이터로 지정\n",
    "    # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))\n",
    "        print('===='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/30 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/30 Cost: 0.614853 Accuracy 66.67%\n",
      "Epoch   20/30 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/30 Cost: 0.373145 Accuracy 83.33%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "\n",
    "# nn.Sequential()은 여러 함수들을 연결해주는 역할\n",
    "model = nn.Sequential(\n",
    "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
    "   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
    ")\n",
    "\n",
    "# 예측값: model(x_train)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 30\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [소트프맥스 함수 구현 -다중클래스 분류]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2328, 0.1150, 0.2485, 0.1661, 0.2375],\n",
      "        [0.1990, 0.1746, 0.2133, 0.1903, 0.2228],\n",
      "        [0.1709, 0.2002, 0.1985, 0.2232, 0.2072]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True) # 데이터(3개 샘플, 5개 클래스)\n",
    "hypothesis = F.softmax(z, dim=1) # 행기준(데이터 기준) 확률 계산\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4574, -2.1628, -1.3922, -1.7949, -1.4377],\n",
       "        [-1.6144, -1.7452, -1.5452, -1.6593, -1.5013],\n",
       "        [-1.7669, -1.6085, -1.6170, -1.4995, -1.5740]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(z, dim=1)) # softmax결과값이 predicted value이다.(hypothesis)\n",
    "\n",
    "# High level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4574, -2.1628, -1.3922, -1.7949, -1.4377],\n",
       "        [-1.6144, -1.7452, -1.5452, -1.6593, -1.5013],\n",
       "        [-1.7669, -1.6085, -1.6170, -1.4995, -1.5740]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 4])\n",
      "tensor([[2],\n",
      "        [4],\n",
      "        [4]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor(1.4892, grad_fn=<MeanBackward0>) tensor(1.4892, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)\n",
    "print(y.unsqueeze(1))\n",
    "\n",
    "\n",
    "# 실제 값 만들기\n",
    "\n",
    "# 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
    "y_one_hot = torch.zeros_like(hypothesis) \n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) \n",
    "# unsqueeze는 index-해당 인덱스에\n",
    "# 해당하는 행(첫번째 인자로 정의)마다 1(3번째 인자로 정의)을 넣는다.\n",
    "print(y_one_hot)\n",
    "\n",
    "\n",
    "# 포스트멕스 함수 구현\n",
    "# Low level\n",
    "# 첫번째 수식\n",
    "a=(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "# 두번째 수식\n",
    "b=(y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4892, grad_fn=<NllLossBackward>) tensor(1.4892, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# High level\n",
    "# 세번째 수식\n",
    "c=F.nll_loss(F.log_softmax(z, dim=1), y)\n",
    "\n",
    "# 네번째 수식\n",
    "d=F.cross_entropy(z, y)\n",
    "\n",
    "print(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.366217\n",
      "Epoch  100/1000 Cost: 0.722726\n",
      "Epoch  200/1000 Cost: 0.637564\n",
      "Epoch  300/1000 Cost: 0.578576\n",
      "Epoch  400/1000 Cost: 0.527363\n",
      "Epoch  500/1000 Cost: 0.479315\n",
      "Epoch  600/1000 Cost: 0.432700\n",
      "Epoch  700/1000 Cost: 0.386693\n",
      "Epoch  800/1000 Cost: 0.340930\n",
      "Epoch  900/1000 Cost: 0.295757\n",
      "Epoch 1000/1000 Cost: 0.255350\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    #prediction = model(x_train)\n",
    "    prediction=model.forward(x_train)\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [mnist 분류]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "# false는 cuda version 10.0이상+ 파이토치 1.3.0이상에서 자주 나온다.\n",
    "# 그러면 토치버전을 1.2.0을 깔아서 test하면 true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 고정\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "training_epochs = 15 # 전체데이터를 15번 학습시킨다.\n",
    "batch_size = 100 # 한 번에 100개 이미지를 동시에 인풋으로 넣어 학습함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.datasets.dsets.MNIST이용, MNIST dataset불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "data_loader = DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size, # 배치 크기는 100\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(784, 10, bias=True).to(device)\n",
    "# to('cuda')는 gpu사용시 필수, cpu쓰는 나는 안써도 무방."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss().to(device) # 내부적으로 소프트맥스 함수를 포함하고 있음.\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.534912527\n",
      "Epoch: 0002 cost = 0.359308630\n",
      "Epoch: 0003 cost = 0.331088215\n",
      "Epoch: 0004 cost = 0.316574216\n",
      "Epoch: 0005 cost = 0.307130307\n",
      "Epoch: 0006 cost = 0.300207853\n",
      "Epoch: 0007 cost = 0.294897318\n",
      "Epoch: 0008 cost = 0.290830463\n",
      "Epoch: 0009 cost = 0.287419587\n",
      "Epoch: 0010 cost = 0.284589082\n",
      "Epoch: 0011 cost = 0.281816214\n",
      "Epoch: 0012 cost = 0.279919654\n",
      "Epoch: 0013 cost = 0.277836859\n",
      "Epoch: 0014 cost = 0.276022345\n",
      "Epoch: 0015 cost = 0.274443179\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs): # 앞서 training_epochs의 값은 15로 지정함.\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서가 된다.\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수.\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8867999911308289\n",
      "Label:  8\n",
      "Prediction:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\pytorch_test\\lib\\site-packages\\torchvision\\datasets\\mnist.py:63: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\user\\Anaconda3\\envs\\pytorch_test\\lib\\site-packages\\torchvision\\datasets\\mnist.py:53: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN5ElEQVR4nO3df4xV9ZnH8c+zWmIyHZWBQYk1Cxb+0DQsbcYJhE1DU7cRjYHGWEu0YZNJqD9I2qQYtcYU/9AYsthopqmhSphdWWoTSuAPohjSxGAiYSRUcMmuv1ig/JiLRpn6i53h2T/msDvinO8d7zn3nluf9yu5ufee5545T274cO4933vO19xdAL76/q7qBgC0BmEHgiDsQBCEHQiCsANBXNzKjU2fPt1nzZrVyk0CoRw+fFinT5+2iWqFwm5mN0p6UtJFkp5x98dTr581a5YGBweLbBJAQk9PT26t4Y/xZnaRpN9IWiLpOknLzey6Rv8egOYq8p29V9Jb7v6Ou5+V9HtJS8tpC0DZioT9KklHxz0/li37HDNbaWaDZjZYq9UKbA5AEUXCPtFBgC/89tbd17t7j7v3dHd3F9gcgCKKhP2YpKvHPf+GpOPF2gHQLEXCvlfSXDObbWZTJP1Y0vZy2gJQtoaH3tx9xMxWSXpRY0NvG9z9jdI6A1CqQuPs7r5D0o6SegHQRPxcFgiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAKzeIKtLOzZ8/m1qZMmdLCTtpDobCb2WFJw5JGJY24e08ZTQEoXxl79u+5++kS/g6AJuI7OxBE0bC7pJ1m9pqZrZzoBWa20swGzWywVqsV3ByARhUN+yJ3/46kJZLuNbPvXvgCd1/v7j3u3tPd3V1wcwAaVSjs7n48ux+StFVSbxlNAShfw2E3sw4z6zz/WNIPJB0sqzEA5SpyNP4KSVvN7Pzf+Xd3f6GUrgBJw8PDyXp/f3+yvmXLltzaNddck1y3s7MzWX/qqaeS9Y6OjmS9Cg2H3d3fkfQPJfYCoIkYegOCIOxAEIQdCIKwA0EQdiAITnFFUx04cCC3tmTJkuS6J0+eTNZHR0eT9WxYeEL79u1LruvuyfrAwECyPjIykqxXgT07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBODuS6o03HzlyJFlfuHBhbi01Di5Jd999d7Je7zTVefPm5dY++uij5Lq33nprsv70008n6+2IPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4O5L27t2brC9YsCBZv/zyy3Nre/bsSa47d+7cZL2ec+fO5dZmz56dXHfOnDnJel9fX0M9VYk9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTh7cMePH0/WU+ejS1JXV1eyvmbNmtxa0XH0M2fOJOsPPfRQbu3o0aPJdS+77LJk/b333kvWp02blqxXoe6e3cw2mNmQmR0ct6zLzF4yszez+6nNbRNAUZP5GL9R0o0XLHtA0i53nytpV/YcQBurG3Z3f1nS+xcsXirp/Pw3A5KWldsWgLI1eoDuCnc/IUnZ/Yy8F5rZSjMbNLPBWq3W4OYAFNX0o/Huvt7de9y9p7u7u9mbA5Cj0bCfMrOZkpTdD5XXEoBmaDTs2yWtyB6vkLStnHYANEvdcXYz2yxpsaTpZnZM0q8kPS7pD2bWJ+mIpNua2SSaJ3XOt1T/uvGrV69O1u+5557cWr1rt6fWlaQXX3wxWR8ayv/A2dvbm1x37dq1yXpnZ2ey3o7qht3dl+eUvl9yLwCaiJ/LAkEQdiAIwg4EQdiBIAg7EASnuKKQ/v7+ZD11KeqtW7cW2vb111+frD/33HO5tRtuuKHQtv8WsWcHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZw+u3iWV66l3KeoXXnght7Z48eLkuqlxckmaMSP3amiSpIsv5p/3eOzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIBiK/Aj7++OPc2pNPPplc9+GHHy67nc9JTdl83333NXXb+Dz27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsbeDdd99N1rdt25asP/LII7m1Dz/8MLnuHXfckazfdlt6Nu5Vq1Yl64899lhura+vL7luV1dXso4vp+6e3cw2mNmQmR0ct2yNmf3FzPZnt5ua2yaAoibzMX6jpBsnWP5rd5+f3XaU2xaAstUNu7u/LOn9FvQCoImKHKBbZWavZx/zp+a9yMxWmtmgmQ3WarUCmwNQRKNh/62kb0qaL+mEpHV5L3T39e7e4+493d3dDW4OQFENhd3dT7n7qLufk/Q7Sb3ltgWgbA2F3cxmjnv6Q0kH814LoD3UHWc3s82SFkuabmbHJP1K0mIzmy/JJR2W9NPmtdj+hoeHk/XVq1cn6xs3bkzWr7zyymR97dq1ubU777wzue4ll1ySrJtZsl7vq9miRYtya/XeN8bZy1U37O6+fILFzzahFwBNxM9lgSAIOxAEYQeCIOxAEIQdCIJTXDOfffZZsn7XXXfl1lLTEkvSp59+mqxv2LAhWV+2bFmy3tHRkawXMTIykqzv2ME5UH8r2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBBhxtk/+eSTZL3eWPfAwEBubfnyiU4M/H+pSz1L0pw5c5L1Zqr3+4LNmzcn648++miyfumll+bWmvn7AHwRe3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCCLMOPv999+frG/atClZ3717d25t4cKFyXXrXY65ntOnTyfrb7/9dm7tlVdeSa77xBNPJOsnT55M1utN6fzMM8/k1jo7O5Prolzs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiDDj7P39/cn6tGnTkvUPPvggt3bLLbck1x0dHU3W69m5c2ey7u65tWuvvTa57ooVK5L122+/PVmfN29eso72UXfPbmZXm9mfzOyQmb1hZj/LlneZ2Utm9mZ2P7X57QJo1GQ+xo9I+oW7XytpgaR7zew6SQ9I2uXucyXtyp4DaFN1w+7uJ9x9X/Z4WNIhSVdJWirp/LWaBiQta1KPAErwpQ7QmdksSd+WtEfSFe5+Qhr7D0HSjJx1VprZoJkN1mq1gu0CaNSkw25mX5e0RdLP3f3MZNdz9/Xu3uPuPd3d3Y30CKAEkwq7mX1NY0Hf5O5/zBafMrOZWX2mpKHmtAigDHWH3mzs/MxnJR1y9/HnQ26XtELS49n9tqZ0WJJXX301WV+3bl2ynrqUdNFLIt98883J+oMPPpisT5kyJbe2YMGChnrCV89kxtkXSfqJpANmtj9b9kuNhfwPZtYn6Yik9InNACpVN+zuvltS3tUXvl9uOwCahZ/LAkEQdiAIwg4EQdiBIAg7EESYU1x7e3uT9eeff75FnQDVYM8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANB1A27mV1tZn8ys0Nm9oaZ/SxbvsbM/mJm+7PbTc1vF0CjJjNJxIikX7j7PjPrlPSamb2U1X7t7v/SvPYAlGUy87OfkHQiezxsZockXdXsxgCU60t9ZzezWZK+LWlPtmiVmb1uZhvMbGrOOivNbNDMBmu1WrFuATRs0mE3s69L2iLp5+5+RtJvJX1T0nyN7fnXTbSeu6939x537+nu7i7eMYCGTCrsZvY1jQV9k7v/UZLc/ZS7j7r7OUm/k5SeORFApSZzNN4kPSvpkLs/MW75zHEv+6Gkg+W3B6Askzkav0jSTyQdMLP92bJfSlpuZvMluaTDkn7ahP4AlGQyR+N3S7IJSjvKbwdAs/ALOiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBDm7q3bmFlN0n+PWzRd0umWNfDltGtv7dqXRG+NKrO3v3f3Ca//1tKwf2HjZoPu3lNZAwnt2lu79iXRW6Na1Rsf44EgCDsQRNVhX1/x9lPatbd27Uuit0a1pLdKv7MDaJ2q9+wAWoSwA0FUEnYzu9HM/tPM3jKzB6roIY+ZHTazA9k01IMV97LBzIbM7OC4ZV1m9pKZvZndTzjHXkW9tcU03olpxit976qe/rzl39nN7CJJ/yXpnyQdk7RX0nJ3/4+WNpLDzA5L6nH3yn+AYWbflfRXSf/q7t/Klq2V9L67P579RznV3e9vk97WSPpr1dN4Z7MVzRw/zbikZZL+WRW+d4m+fqQWvG9V7Nl7Jb3l7u+4+1lJv5e0tII+2p67vyzp/QsWL5U0kD0e0Ng/lpbL6a0tuPsJd9+XPR6WdH6a8Urfu0RfLVFF2K+SdHTc82Nqr/neXdJOM3vNzFZW3cwErnD3E9LYPx5JMyru50J1p/FupQumGW+b966R6c+LqiLsE00l1U7jf4vc/TuSlki6N/u4ismZ1DTerTLBNONtodHpz4uqIuzHJF097vk3JB2voI8Jufvx7H5I0la131TUp87PoJvdD1Xcz/9pp2m8J5pmXG3w3lU5/XkVYd8raa6ZzTazKZJ+LGl7BX18gZl1ZAdOZGYdkn6g9puKerukFdnjFZK2VdjL57TLNN5504yr4veu8unP3b3lN0k3aeyI/NuSHqqih5y+rpH05+z2RtW9SdqssY91/6OxT0R9kqZJ2iXpzey+q416+zdJByS9rrFgzayot3/U2FfD1yXtz243Vf3eJfpqyfvGz2WBIPgFHRAEYQeCIOxAEIQdCIKwA0EQdiAIwg4E8b/E3CyLg7Q4kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트 데이터를 사용하여 모델을 테스트한다.\n",
    "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [간단 CNN으로 데이터 분류]\n",
    "\n",
    "1. 데이터의 shape\n",
    "\n",
    "* 3차원: (5,5,1) = 1개의 채널(depth)을 가지는 이미ㅣ\n",
    "* 4차원: (20,1,5,5) = 20개의 이미지 배치, 한개의 이미지는 (5,5,1) 모양을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서의 크기 : torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 배치 크기 × 채널 × 높이(height) × 너비(widht)의 크기의 텐서를 선언\n",
    "inputs = torch.Tensor(1, 1, 28, 28)\n",
    "print('텐서의 크기 : {}'.format(inputs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# (입력체널수,output 체널수, 커널필터 사이즈,패딩사이즈)\n",
    "conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "print(conv1)\n",
    "# 1채널 짜리를 입력받아서 32채널을 뽑아내는데 커널 사이즈는 3이고 패딩은 1입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "print(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "# 정수 하나를 인자로 넣으면 커널 사이즈와 스트라이드가 둘 다 해당값으로 지정\n",
    "pool = nn.MaxPool2d(2)\n",
    "print(pool) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "out = conv1(inputs) # 체널이 1에서 32로 증가함. 가로세로는 이전과 동일\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "out = pool(out) # 멕스풀링을 적용하면 가로세로가 작아진다.\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "out = conv2(out) # 2번째 필터 적용\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "out = pool(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size의 0: 1\n",
      "output size의 1: 64\n",
      "output size의 2: 7\n",
      "output size의 3: 7\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(\"output size의 \"+str(i) +\":\", out.size(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 차원인 배치 차원은 그대로 두고 나머지는 펼쳐라\n",
    "\n",
    "out = out.view(out.size(0), -1)  # 가세높을 모두 하나로 합친다.\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(3136, 10) # input_dim = 3,136, output_dim = 10\n",
    "out = fc(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if gpu 이용 가능시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
    "                          train=True, # True를 지정하면 훈련 데이터로 다운로드\n",
    "                          transform=transforms.ToTensor(), # 텐서로 변환\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
    "                         train=False, # False를 지정하면 테스트 데이터로 다운로드\n",
    "                         transform=transforms.ToTensor(), # 텐서로 변환\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader을 이용하여 train데이터의 배치사이즈를 입력해 그 크기만큼 나눠준다.\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 첫번째층\n",
    "        # ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 두번째층\n",
    "        # ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
    "\n",
    "        # 전결합층 한정으로 가중치 초기화\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x): # x는 train data\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : 600\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.00726089068\n",
      "[Epoch:    2] cost = 0.00566023262\n",
      "[Epoch:    3] cost = 0.00564216357\n",
      "[Epoch:    4] cost = 0.0032782082\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "for epoch in range(4):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y느 ㄴ레이블.\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # 초기화\n",
    "        hypothesis = model(X) # 예측값 구함\n",
    "        cost = criterion(hypothesis, Y) # 실제-예측값 차이\n",
    "        cost.backward() # cost함수 미분\n",
    "        optimizer.step() # 미분한 만큼 opt를 이동\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "# 학습을 진행하지 않을 것이므로 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [더 깊은 conv net]\n",
    "\n",
    "- 입력 이미지가 (7,7,3) 으로 체널이 3개라면, 적용되는 filter kernel의 체널모양은 (?,?,3)이어야 함.\n",
    "- 위의 경우, 입력 이미지의 체널 3개인 c1,c2,c3에 kernel의 체널인 k1,k2,k3이 각각 적용되어 c1* k1, c2* k2, c3* k3 총 3가지 피쳐맵이 나오게 된다.\n",
    "- 3가지 피쳐맵을 모두 더하여 하나의 (@,@) 피쳐맵이 도출된다.\n",
    "- 만약, 이미지에 적용되는 kernel이 2개이상이면, 도출되는 피쳐맵의 shape은 (@,@,kernel의 수) 로 나온다.  \n",
    "\n",
    "\n",
    "* 깊은 CNN 모델의 이해: 아래 코드에서 구현할 모델 \n",
    "    - 1번 레이어: conv layer- relu, max pooling\n",
    "    - 2번 레이어: conv layer- relu, max pooling\n",
    "    - 3번 레이어: conv layer- relu, max pooling\n",
    "    - 4번 레이어: fc layer- relu\n",
    "    - 5번 레이어: fc layer- softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.keep_prob = 0.5\n",
    "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # L3 ImgIn shape=(?, 7, 7, 64)\n",
    "        #    Conv      ->(?, 7, 7, 128)\n",
    "        #    Pool      ->(?, 4, 4, 128)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
    "\n",
    "        # L4 FC 4x4x128 inputs -> 625 outputs\n",
    "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=1 - self.keep_prob))\n",
    "        # L5 Final FC 625 inputs -> 10 outputs\n",
    "        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
    "        out = self.layer4(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [자연어처리] \n",
    "\n",
    "1. 토큰화: 주어진 텍스트를 단어 또는 문자 단위로 자르는 것\n",
    "    -  spaCy와 NLTK 패키지 이용 or 파이썬 split함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = \"A Dog Run back corner near spare bedrooms\"\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "tok=spacy_en.tokenizer(en_text)\n",
    "print(len(tok))\n",
    "print(type(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(en_text): # 영어문장을 받으면 \n",
    "    return [tok.text for tok in spacy_en.tokenizer(en_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
     ]
    }
   ],
   "source": [
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
    "print(kor_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 - 형태소 분석기 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
